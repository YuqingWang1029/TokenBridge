<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TokenBridge: Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation">
  <meta name="keywords" content="TokenBridge, Visual Generation, Autoregressive Models, Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TokenBridge: Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QCJY998LVV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-QCJY998LVV');
  </script>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* Global background setting */
    body, html {
      background-color: #ffffff;
      font-family: 'Google Sans', 'Noto Sans', sans-serif;
    }
    
    /* 修复标题行数问题 */
    .publication-title {
      font-size: 2.5rem !important; /* 适当减小字号 */
      line-height: 1.2 !important; /* 减小行高 */
      width: 100%;
      max-width: 900px;
      margin: 0 auto;
    }
    
    /* 内容卡片样式 - 统一的蓝色背景 */
    .content-card {
      background-color: #f8f9ff;
      border-radius: 8px;
      padding: 25px;
      margin-bottom: 30px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.05);
      transition: all 0.3s ease;
    }
    
    .content-card:hover {
      box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    /* Enhanced section styling */
    .section {
      padding: 3rem 1.5rem;
    }
    
    /* 居中的标题样式 */
    .title-centered {
      position: relative;
      text-align: center;
      color: #0074D9;
      margin-bottom: 2.5rem;
      font-weight: 600;
      letter-spacing: 0.5px;
    }
    
    .title-centered::after {
      content: "";
      position: absolute;
      bottom: -10px;
      left: 50%;
      transform: translateX(-50%);
      width: 80px;
      height: 3px;
      background-color: #0074D9;
      border-radius: 2px;
    }
    
    /* 贡献项目样式 - 修改文字颜色为正常 */
    .contribution-item {
      color: #333; /* 改为正常颜色 */
      font-weight: 500; /* 稍微减少粗体 */
      padding: 15px 20px;
      background-color: #f8f9ff;
      border-radius: 8px;
      margin-bottom: 15px !important;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }

    .contribution-item:hover {
      background-color: #f0f4ff;
      transform: translateX(5px);
    }
    
    /* Image styling */
    .content img, .method-figure {
      width: 100%;
      max-width: 900px;
      margin: 0 auto 0.5rem auto;
      display: block;
      box-shadow: none;
      border-radius: 8px; 
      background-color: #ffffff;
      transition: transform 0.3s ease;
    }
    
    /* Smaller figures */
    .method-figure.small-figure {
      width: 60%;
      max-width: 700px;
    }

    /* Full width figures */
    .full-width-figure {
      width: 100% !important;
      max-width: none !important;
      background-color: #ffffff;
    }
    
    /* Navigation bar styling */
    .navbar {
      position: sticky;
      top: 0;
      z-index: 100;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      background-color: rgba(255, 255, 255, 0.95);
      transition: all 0.3s ease;
    }
    
    /* Text emphasis styling */
    strong {
      color: #0074D9;
      font-weight: 600;
    }
    
    /* 图片说明文字与图片之间距离控制 */
    .figure-caption {
      margin-top: 0.8rem !important;
      padding-top: 0 !important;
      font-size: 0.95rem;
      line-height: 1.5;
      color: #444;
    }
    
    /* 确保所有SVG图像有白色背景 */
    img[src$=".svg"] {
      background-color: #ffffff !important;
    }
    
    /* 图片宽度设置 */
    .full-width-container {
      width: 100%;
      max-width: 1000px; /* 控制容器宽度 */
      margin: 0 auto;
    }
    
    /* 标准宽度的图片 */
    .full-width-comparison {
      width: 100%;
      max-width: 100%;
      border-radius: 8px;
    }
    
    /* 图片说明宽度 */
    .caption-container {
      width: 100%;
      max-width: 1000px; /* 与图片宽度一致 */
      margin: 0 auto;
    }
    
    .standard-figure {
      max-width: 900px;
      width: 100%;
    }
    
    /* 调整Token Dilemma部分 */
    .dilemma-container {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin-top: 15px;
      justify-content: space-between;
    }
    
    .dilemma-side {
      flex: 1;
      min-width: 250px;
      background-color: #ffffff;
      border-radius: 8px;
      padding: 20px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
      transition: all 0.2s ease;
    }
    
    .dilemma-side:hover {
      box-shadow: 0 3px 8px rgba(0,0,0,0.15);
    }
    
    .dilemma-side h4 {
      color: #0074D9;
      font-weight: 600;
      margin-bottom: 15px;
      font-size: 1.2rem;
      position: relative;
      padding-bottom: 8px;
    }
    
    .dilemma-side h4::after {
      content: "";
      position: absolute;
      bottom: 0;
      left: 0;
      width: 50px;
      height: 2px;
      background-color: #0074D9;
    }
    
    .dilemma-side ul {
      margin-left: 20px;
      line-height: 1.6;
    }
    
    .dilemma-side li {
      margin-bottom: 8px;
    }
    
    /* 方法标题样式 */
    .method-title {
      font-size: 1.5rem;
      color: #0074D9;
      font-weight: 600;
      margin-bottom: 20px;
      text-align: center;
    }
    
    /* 代码格式 */
    code {
      background-color: transparent !important;
      color: #333 !important;
      padding: 0 !important;
      font-family: monospace;
    }
    
    pre {
      background-color: transparent !important;
      padding: 0 !important;
      border: none !important;
      margin: 0 !important;
    }
    
    /* 贡献点中的蓝色文字覆盖 */
    .contribution-item strong,
    .contribution-item a,
    .contribution-item p {
      color: #333 !important;
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://yuqingwang1029.github.io/Loong-video/">
            Loong
          </a>
          <a class="navbar-item" href="https://yuqingwang1029.github.io/PAR-project/">
            PAR
          </a>
          <a class="navbar-item" href="#">
            TokenBridge
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TokenBridge: Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=QC7nNe0AAAAJ&hl=zh-CN">Yuqing Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=xXMj6_EAAAAJ&hl=zh-CN">Zhijie Lin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://tyshiwo1.github.io/">Yao Teng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yuanzhi-zhu.github.io/about/">Yuanzhi Zhu</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://renshuhuai-andy.github.io/">Shuhuai Ren</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=zh-CN">Jiashi Feng</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>1*</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>ByteDance Seed,</span>
            <span class="author-block"><sup>3</sup>École Polytechnique,</span>
            <span class="author-block"><sup>4</sup>Peking University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.16430" target="_blank">
                  <span class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yuqingwang1029/TokenBridge" target="_blank">
                  <span class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TOKEN REPRESENTATION DILEMMA SECTION -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">Token Representation Dilemma</h2>
        <div class="content">
          <div class="content-card" data-aos="fade-up" data-aos-delay="100">
            <p class="is-size-5">
              Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. However, a fundamental dilemma exists in token representation:
            </p>
            
            <div class="dilemma-container">
              <div class="dilemma-side">
                <h4>Discrete Tokens</h4>
                <ul>
                  <li>😊 Enable straightforward modeling with standard cross-entropy loss</li>
                  <li>😢 Suffer from information loss due to training quantization</li>
                  <li>😢 Face tokenizer training instability issues</li>
                  <li>😢 Limited vocabulary size restricts representational capacity</li>
                </ul>
              </div>
              
              <div class="dilemma-side">
                <h4>Continuous Tokens</h4>
                <ul>
                  <li>😊 Better preserve rich visual details</li>
                  <li>😊 Avoid quantization bottlenecks during training</li>
                  <li>😢 Require complex distribution modeling (diffusion or GMM)</li>
                  <li>😢 Complicate the generation pipeline with specialized components</li>
                </ul>
              </div>
            </div>
            
            <p class="is-size-5 mt-4">
              <strong>How do we bridge this gap?</strong>
            </p>
            
            <ol class="mt-2 ml-5">
              <li class="mb-2">We <strong>decouple discretization from the tokenizer training process</strong> through post-training quantization that directly obtains discrete tokens from pretrained continuous representations, enabling seamless conversion between token types.</li>
              <li class="mb-2">Our approach <strong>bridges the quality gap</strong> between discrete and continuous methods, achieving continuous-level visual quality while maintaining the modeling simplicity of discrete approaches - harnessing the strengths of both approaches.</li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Comparison Section - 受控宽度 -->
<section class="section">
  <div class="full-width-container">
    <h2 class="title is-3 title-centered" data-aos="fade-up">Comparison of Different AR Approaches</h2>
    <div class="content" data-aos="fade-up" data-aos-delay="100">
      <img src="./static/images/compare.svg" alt="Method Comparison" class="full-width-comparison" style="background-color: #ffffff;">
      
      <div class="caption-container">
        <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
          <p>
             <strong>(a) Traditional discrete tokenization</strong> incorporates quantization during training, resulting in tokenizer training instability and limited vocabulary size that restricts representational capacity. <strong>(b) Hybrid continuous AR models</strong> preserve rich visual information but need complex distribution modeling (diffusion or GMM) beyond standard categorical prediction. <strong>(c) Our approach</strong> bridges these paradigms by applying post-training quantization to pretrained continuous features, maintaining the high representational capacity of continuous tokens while enabling simple autoregressive modeling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image with controlled width -->
<section class="section">
  <div class="full-width-container">
    <div class="content" data-aos="fade-up">
      <img src="./static/images/vis.svg" alt="TokenBridge Teaser" class="full-width-comparison" style="background-color: #ffffff;">
      
      <div class="caption-container">
        <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="200">
          <p>
            TokenBridge combines the representational capacity of continuous tokens with the modeling simplicity of discrete approaches for high-quality visual generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">Abstract</h2>
        
        <div class="content has-text-justified" data-aos="fade-up" data-aos-delay="100">
          <p>
            Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens.
          </p>
          <p>
            To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Main Contributions Section - 修改文字颜色为正常 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">Main Contributions</h2>
        <div class="content">
          <ul style="list-style-type: none; padding: 0;">
            <li class="is-size-5 mb-4 has-text-left contribution-item" data-aos="fade-up" data-aos-delay="100">A novel paradigm that bridges continuous and discrete token representations, achieving continuous level visual quality with the standard autoregressive cross-entropy loss.</li>
            <li class="is-size-5 mb-4 has-text-left contribution-item" data-aos="fade-up" data-aos-delay="200">A training-free quantization approach that transforms pretrained VAE features into discrete tokens without the optimization instabilities of conventional discrete tokenizers.</li>
            <li class="is-size-5 mb-4 has-text-left contribution-item" data-aos="fade-up" data-aos-delay="300">An efficient dimension-wise autoregressive prediction mechanism that handles exponentially large token spaces.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">Method</h2>
        
        <!-- Post-Training Quantization -->
        <h3 class="method-title mt-6" data-aos="fade-up">Post-Training Quantization</h3>
        <div class="full-width-container" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/quant.svg" alt="Post-Training Quantization Process" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
                <p>
                  <strong>Illustration of our post-training quantization process.</strong> The top row shows the pretrained continuous VAE tokenizer, mapping an input image to continuous latent features and reconstructing it through the decoder. Our post-training quantization process (middle) transforms these continuous features into discrete tokens by independently quantizing each channel dimension. The bottom-left shows how our approach preserves the original Gaussian-like distribution (purple curve) in discretized form (purple histogram). The right portion demonstrates the de-quantization process that maps indices back to continuous values for decoding.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Efficient Large-Vocabulary Token Modeling -->
        <h3 class="method-title mt-6" data-aos="fade-up">Efficient Large-Vocabulary Token Modeling</h3>
        <div class="content" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/token.svg" alt="Dimension-wise Autoregressive Prediction" class="method-figure small-figure" style="background-color: #ffffff;">
          
            <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
              <p>
                <strong>Our autoregressive generation process.</strong> At the spatial level, our model autoregressively generates tokens conditioning on previous positions. For each spatial location, we apply dimension-wise sequential prediction to efficiently handle the large token space. This approach decomposes the modeling of each token into a series of smaller classification problems while preserving essential inter-dimensional dependencies.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experiments Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">Experiments</h2>
        
        <!-- Main Results -->
        <h3 class="method-title mt-6" data-aos="fade-up">Main Results</h3>
        <div class="content" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/results.png" alt="Quantitative Comparison" class="standard-figure" style="background-color: #ffffff;">
          
            <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
              <p>
                <strong>Comparison of visual generation methods on ImageNet 256×256.</strong> Our model achieves comparable performance to the best continuous token approach (MAR) while using standard categorical prediction in autoregressive modeling.
              </p>
            </div>
          </div>
        </div>
        
        <!-- Properties of Our Tokenizer -->
        <h3 class="method-title mt-6" data-aos="fade-up">Properties of Our Tokenizer</h3>
        <div class="full-width-container" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/recons.svg" alt="Reconstruction Quality Comparison" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
                <p>
                  <strong>Reconstruction quality of typical continuous and discrete tokenizers.</strong> Our method achieves reconstruction quality comparable to continuous VAE, preserving more fine details than traditional discrete tokenizers, especially in text and facial features.
                </p>
              </div>
            </div>
          </div>
          
          <!-- Different granularity reconstruction results -->
          <div class="has-text-centered mt-6" data-aos="fade-up" data-aos-delay="200">
            <img src="./static/images/recons_b.svg" alt="Different Granularity Reconstruction Results" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="250">
                <p>
                  <strong>Different quantization granularity reconstruction results.</strong> Visual comparison showing reconstructions at varying quantization levels. While global structure remains preserved across all quantization levels, finer quantization (higher B values) better maintains details in textures and edges.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Properties of Our Generator -->
        <h3 class="method-title mt-6" data-aos="fade-up">Properties of Our Generator</h3>
        <div class="full-width-container" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/parallel.svg" alt="Token Prediction Strategy Comparison" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
                <p>
                  <strong>Token Prediction Strategy.</strong> Comparison of dimension-wise token prediction approaches. Top: Parallel prediction produces blurry, inconsistent images. Bottom: Our autoregressive approach sequentially predicts token dimensions, generating coherent, high-quality images. This highlights the interdependence of token dimensions and they cannot be predicted independently.
                </p>
              </div>
            </div>
          </div>
          
          <div class="has-text-centered mt-6" data-aos="fade-up" data-aos-delay="200">
            <img src="./static/images/confidence.svg" alt="Confidence-guided Generation" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="250">
                <p>
                  <strong>Generation guided by token confidence.</strong> Our discrete token approach enables confidence-guided generation, producing clean foreground objects against simple backgrounds by prioritizing high-confidence tokens. This provides an advantage over continuous tokens, which lack explicit token-level confidence scores.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- More Visualization Results -->
        <h3 class="method-title mt-6" data-aos="fade-up">More Visualization Results</h3>
        <div class="full-width-container" data-aos="fade-up" data-aos-delay="100">
          <div class="has-text-centered">
            <img src="./static/images/more.svg" alt="ImageNet Generation Results" class="full-width-comparison" style="background-color: #ffffff;">
          
            <div class="caption-container">
              <div class="content has-text-justified figure-caption" data-aos="fade-up" data-aos-delay="150">
                <p>
                  <strong>Class-conditional generation results on ImageNet.</strong> Our approach achieves high-quality generation with fine details and realistic textures across diverse object categories.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 title-centered" data-aos="fade-up">BibTeX</h2>
        <div data-aos="fade-up" data-aos-delay="100">
          <pre><code>coming soon~
</code></pre>
        </div>

        <h3 class="method-title mt-6" data-aos="fade-up">Acknowledgment</h3>
        <div data-aos="fade-up" data-aos-delay="150">
          <p>The authors are grateful to <a href="https://www.tianhongli.me/">Tianhong Li</a> for helpful discussions on MAR and to <a href="https://enjoyyi.github.io/">Yi Jiang</a>, <a href="https://difanzou.github.io/">Prof. Difan Zou</a>, and <a href="https://yujinhanml.github.io/">Yujin Han</a> for valuable feedback on the early version of this work.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template is borrowed from <a href="https://nerfies.github.io/">Nerfies</a> under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script>
  // Initialize AOS animation library
  document.addEventListener('DOMContentLoaded', function() {
    AOS.init({
      duration: 800,
      easing: 'ease',
      once: true
    });
  });

  // Handle navbar burger menu for mobile
  document.addEventListener('DOMContentLoaded', () => {
    const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
    if ($navbarBurgers.length > 0) {
      $navbarBurgers.forEach(el => {
        el.addEventListener('click', () => {
          const target = el.dataset.target;
          const $target = document.getElementById(target);
          el.classList.toggle('is-active');
          $target.classList.toggle('is-active');
        });
      });
    }
  });
</script>

</body>
</html>
